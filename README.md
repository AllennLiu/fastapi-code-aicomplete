# FastAPI Code AI Complete

ä½¿ç”¨ `FastAPI` æ­é…é è¨“ç·´ `CodeGeeX2 + GLM-4` æ¨¡å‹ï¼Œé€šé `AI` æ™ºèƒ½ç”Ÿæˆä»£ç¢¼

ç›®å‰æ¸¬è©¦èµ·ä¾†ï¼Œ**æ•´é«”åŠŸèƒ½éå¸¸å¼·å¤§**ï¼Œå°æ–¼ç¡¬ä»¶ç’°å¢ƒæ²’é‚£éº¼å¯Œè£•çš„é–‹ç™¼äººå“¡å¾ˆå‹å¥½ã€‚

ç›®å‰å·²æ•´åˆäº† `CodeGeeX2-6B` + `GLM-4-9B-Chat` é‡åŒ–æ¨¡å‹ï¼Œä½¿é€™ 2 å€‹ç”±**æ¸…åå¤§å­¦ KEG æ™ºè°±**å‰å¤§çš„ä½œå“å¾ˆå¥½çš„åˆä½µä½¿ç”¨ï¼

> **Docker** é¡åƒä¾†æ‰“åŒ…æ•´å€‹ **FastAPI** æœå‹™æˆä¸€å€‹ç¨‹åº *(åŒ…å«æ¨¡å‹æ–‡ä»¶å¤§å° > `50G` )*

---
## ğŸ¥ æ•ˆæœæ¼”ç¤º *(Stream)*

### ğŸ‘©â€ğŸ’» `AI` ä¾æŒ‡å®šéœ€æ±‚ç”Ÿæˆä»£ç¢¼ ğŸ”— http://127.0.0.1:7860/copilot/demo

![ai-coding](https://github.com/AllennLiu/fastapi-code-aicomplete/assets/27174570/2978ffa4-e08b-41d7-882e-f83c7011453e)

### ğŸ¤– èˆ‡èŠå¤©æ©Ÿå™¨äººäº’å‹• ğŸ”— http://127.0.0.1:7860/chat/demo

![chat_x16_x10_x2](https://github.com/AllennLiu/fastapi-code-aicomplete/assets/27174570/79fc7243-9d4c-4ce1-bab1-2de5d97e3c98)

---

## ğŸŒ ç’°å¢ƒæº–å‚™

- æœ¬é …ç›®å·²ç¶“å°‡å·²é…ç½®å¥½çš„ç’°å¢ƒé¡åƒï¼Œæ¨åœ¨ `Dockerhub` ä¸Šäº†ï¼Œ**å¤§å° `27G`** *(å…§å«å·²è¼‰å…¥çš„é‡åŒ– `CodeGeeX2-6B` + `ChatGLM3-6B` æ¨¡å‹)*
- é¡åƒé€£çµ ğŸ”— [seven6306/pretrained-model:ai-fastapi-copilot](https://hub.docker.com/repository/docker/seven6306/pretrained-model/tags)
- å¦‚æœæ‚¨è¦æ‰‹å‹• Build **Docker** é¡åƒ `ai-fastapi-copilot` *(ä½¿ç”¨ Docker ä¾†é¿å…ç’°å¢ƒä¾è³´ç­‰å•é¡Œ)*

  ```bash
  docker build --no-cache -t seven6306/pretrained-model:ai-fastapi-copilot . \
    --build-arg "HTTP_PROXY=http://admin:ZD7EdEpF9qCYpDpu@10.99.104.250:8081/" \
    --build-arg "HTTPS_PROXY=http://admin:ZD7EdEpF9qCYpDpu@10.99.104.250:8081/" \
    --build-arg "NO_PROXY=localhost,127.0.0.1,.example.com"
  # æ›ä»£ç† --build-arg æ˜¯æˆ‘å…§éƒ¨ç’°å¢ƒç”¨çš„ï¼Œå¯ä»¥åˆªé™¤
  ```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

å•Ÿå‹• **Web API** æœå‹™ï¼Œé€™è£¡å•Ÿç”¨äº† **`GPU Driver`** è«‹ç¢ºä¿å®‰è£ Docker çš„ç³»çµ±ç’°å¢ƒå·²å®‰è£ `cuda-toolkit`ã€‚

> å¦‚æœç”¨ **CPU** è«‹åˆªé™¤ `--gpus all` èˆ‡ `-e "LOAD_MODEL_DEVICE=gpu"` å…©å€‹åƒæ•¸

```bash
docker run -tid -p 7861:22 -p 7860:7860 \
  --gpus all \
  -e "LOAD_MODEL_DEVICE=gpu" \
  -v /etc/localtime:/etc/localtime:ro \
  -v /root/.ssh:/root/.ssh:ro \
  --ulimit nofile=65535 --privileged=true --restart=always \
  --name copilot seven6306/pretrained-model:ai-fastapi-copilot
```

---

#### åœ¨ **`FastAPI` Docs** ä¸‹å˜—è©¦ http://127.0.0.1:7860/docs

### ğŸ‘¨â€ğŸ’» ä»£ç¢¼ç”Ÿæˆ

#### å¯ä»¥é€‰æ‹©çš„å‚æ•°

- `lang` - ç¨‹åºçš„èªè¨€å¦‚ `Python, JavaScript, Shell, ...`
- `prompt` - æè¿°ç¨‹åºçš„éœ€æ±‚
- `html` - æ˜¯å¦ç›´æ¥è¿”å›ä»£ç¢¼ï¼Œé»˜èªç‚º `false`

```bash
curl -sX POST http://127.0.0.1:7860/copilot/coding \
  -H 'Content-Type: application/json'\
  -d '{ "lang": "Python", "prompt": "å¯«ä¸€å€‹ç¨‹åºåŸ·è¡Œå‘½ä»¤ ipmitool raw 6 1 åˆ¤æ–· 00 åœ¨è¿”å›å€¼ä¸­æ‰“å° Pass ä¸åœ¨å°±æ‰“å° Fail", "html": true }' | python
```

#### å¯ä»¥æ­£å¸¸åŸ·è¡Œ **AI** ç”Ÿæˆçš„ `Python` è…³æœ¬ä¸¦ç¬¦åˆé æœŸï¼

```bash
20 00 02 12 02 8d dd b3 00 18 00 00 00 00 00

20 00 02 12 02 8d dd b3 00 18 00 00 00 00 00

1

Pass
```

![api_demo](https://github.com/AllennLiu/fastapi-code-aicomplete/assets/27174570/752d6d17-47a8-4c89-b31b-b03c962703fe)

---

### ğŸ’¬ é€²è¡ŒèŠå¤©

#### å¯ä»¥é€‰æ‹©çš„å‚æ•°

- `query` - ç”¨æˆ¶è¼¸å…¥ä¿¡æ¯
- `history` - é€²è¡Œ**å¤šè¼ªå¼å°è©±** *(æ•¸çµ„å½¢å¼ `[ dict... ]`)*
- `role` - äº¤äº’è§’è‰²å¦‚ `user, assistant, system, observation`ï¼Œé»˜èªç‚º `user`
- `html` - æ˜¯å¦ç›´æ¥è¿”å›æ©Ÿå™¨äººå›è¦†çš„ä¿¡æ¯ï¼Œé»˜èªç‚º `false`

```bash
curl -X POST http://127.0.0.1:7860/chat/conversation \
  -H 'Content-Type: application/json' -d '{ "query": "ä½ æœ€è¿‘é‚„å¥½å—?" }'
```

```json
{
  "response": "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ã€‚ä½ å‘¢ï¼Ÿ",
  "history": [
    {
      "role": "user",
      "content": "ä½ æœ€è¿‘é‚„å¥½å—?"
    },
    {
      "role": "assistant",
      "metadata": "",
      "content": "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ã€‚ä½ å‘¢ï¼Ÿ"
    }
  ],
  "datetime": "2024-06-14 05:40:02",
  "elapsed_time": 1191.721331
}
```

---

## ğŸ˜µ Troubleshooting

å¦‚ä½•ç›£æ§ GPU åˆ©ç”¨ç‡ï¼Ÿæœ¬åœ°è«‹å…ˆå®‰è£ `cuda-toolkit` å·¥å…·ï¼Œ**Docker Container** `--gpus all` æœƒè‡ªå‹•ä¾è³´æœ¬åœ°çš„é©…å‹•ã€‚

```bash
watch nvidia-smi
```

```bash
Every 2.0s: nvidia-smi                                                                                                                                                                                                                                   Blade-Allen: Fri Jun  7 21:06:49 2024

Fri Jun  7 21:06:49 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.69                 Driver Version: 551.95         CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4070 ...    On  |   00000000:01:00.0 Off |                  N/A |
| N/A   47C    P0             20W /  105W |       0MiB /   8188MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```

---

## ğŸ’¡ åƒè€ƒä¾†æº

- https://github.com/THUDM/CodeGeeX2
- https://github.com/THUDM/GLM-4
- https://github.com/li-plus/chatglm.cpp
